{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a383ad0",
   "metadata": {},
   "source": [
    "# Temperature Forecast Project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d9a8a9",
   "metadata": {},
   "source": [
    "Project Description\n",
    "\n",
    "This data is for the purpose of bias correction of next-day maximum and minimum air temperatures forecast of the LDAPS model operated by the Korea Meteorological Administration over Seoul, South Korea. This data consists of summer data from 2013 to 2017. The input data is largely composed of the LDAPS model's next-day forecast data, in-situ maximum and minimum temperatures of present-day, and geographic auxiliary variables. There are two outputs (i.e. next-day maximum and minimum air temperatures) in this data. Hindcast validation was conducted for the period from 2015 to 2017.\n",
    "\n",
    "Attribute Information:\n",
    "For more information, read [Cho et al, 2020].\n",
    "\n",
    "1. station - used weather station number: 1 to 25\n",
    "\n",
    "2. Date - Present day: yyyy-mm-dd ('2013-06-30' to '2017-08-30')\n",
    "\n",
    "3. Present_Tmax - Maximum air temperature between 0 and 21 h on the present day (Â°C): 20 to 37.6\n",
    "\n",
    "4. Present_Tmin - Minimum air temperature between 0 and 21 h on the present day (Â°C): 11.3 to 29.9\n",
    "\n",
    "5. LDAPS_RHmin - LDAPS model forecast of next-day minimum relative humidity (%): 19.8 to 98.5\n",
    "\n",
    "6. LDAPS_RHmax - LDAPS model forecast of next-day maximum relative humidity (%): 58.9 to 100\n",
    "\n",
    "7. LDAPS_Tmax_lapse - LDAPS model forecast of next-day maximum air temperature applied lapse rate (Â°C): 17.6 to 38.5\n",
    "\n",
    "8. LDAPS_Tmin_lapse - LDAPS model forecast of next-day minimum air temperature applied lapse rate (Â°C): 14.3 to 29.6\n",
    "\n",
    "9. LDAPS_WS - LDAPS model forecast of next-day average wind speed (m/s): 2.9 to 21.9\n",
    "\n",
    "10. LDAPS_LH - LDAPS model forecast of next-day average latent heat flux (W/m2): -13.6 to 213.4\n",
    "\n",
    "11. LDAPS_CC1 - LDAPS model forecast of next-day 1st 6-hour split average cloud cover (0-5 h) (%): 0 to 0.97\n",
    "\n",
    "12. LDAPS_CC2 - LDAPS model forecast of next-day 2nd 6-hour split average cloud cover (6-11 h) (%): 0 to 0.97\n",
    "\n",
    "13. LDAPS_CC3 - LDAPS model forecast of next-day 3rd 6-hour split average cloud cover (12-17 h) (%): 0 to 0.98\n",
    "\n",
    "14. LDAPS_CC4 - LDAPS model forecast of next-day 4th 6-hour split average cloud cover (18-23 h) (%): 0 to 0.97\n",
    "\n",
    "15. LDAPS_PPT1 - LDAPS model forecast of next-day 1st 6-hour split average precipitation (0-5 h) (%): 0 to 23.7\n",
    "\n",
    "16. LDAPS_PPT2 - LDAPS model forecast of next-day 2nd 6-hour split average precipitation (6-11 h) (%): 0 to 21.6\n",
    "\n",
    "17. LDAPS_PPT3 - LDAPS model forecast of next-day 3rd 6-hour split average precipitation (12-17 h) (%): 0 to 15.8\n",
    "\n",
    "18. LDAPS_PPT4 - LDAPS model forecast of next-day 4th 6-hour split average precipitation (18-23 h) (%): 0 to 16.7\n",
    "\n",
    "19. lat - Latitude (Â°): 37.456 to 37.645\n",
    "\n",
    "20. lon - Longitude (Â°): 126.826 to 127.135\n",
    "\n",
    "21. DEM - Elevation (m): 12.4 to 212.3\n",
    "\n",
    "22. Slope - Slope (Â°): 0.1 to 5.2\n",
    "\n",
    "23. Solar radiation - Daily incoming solar radiation (wh/m2): 4329.5 to 5992.9\n",
    "\n",
    "24. Next_Tmax - The next-day maximum air temperature (Â°C): 17.4 to 38.9\n",
    "\n",
    "25. Next_Tmin - The next-day minimum air temperature (Â°C): 11.3 to 29.8T\n",
    "\n",
    "You have to build separate models that can predict the minimum temperature for the next day and the maximum temperature for the next day based on the details provided in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0536f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pt\n",
    "import seaborn as sb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'https://raw.githubusercontent.com/dsrscientist/Dataset2/main/temperature.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ffabcb",
   "metadata": {},
   "source": [
    "We see that in our dataset there are a total of 7752 rows and 25 columns present. We see that right now all the information shown above is in numerical format and has no text data but we will need to investigate on it further. Also the problem statement says that we are suppose to predict two label columns namely \"Next_Tmax\" and \"Next_Tmin\". These target labels contain all continous data values in them so it makes this to be a Regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6b9d6a",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde7215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34008b4",
   "metadata": {},
   "source": [
    "Using the info method we can see that there is only 1 column with object datatype and remaining 24 columns have float datatype values in them. The column which shows object datatype is actually for \"Date\" and we will have to treat it and convert it into numerical format. And as we can see that the null values present are very less compared to non null values so we will drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d6145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c44683",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777478a0",
   "metadata": {},
   "source": [
    "Using the describe method to check the numerical data details. Almost all the columns in our dataset has numerical values in them and it looks like the count, mean, standard deviation, minimum value, 25% quartile, 50% quartile, 75% quartile and maximum value are all properly distributed in terms of data points. However, I do see some outliers and skewness possibility that we will have to confirm with a visual on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date']=pd.to_datetime(df['Date'])\n",
    "df['Day']=df['Date'].apply(lambda x:x.day)\n",
    "df['Month']=df['Date'].apply(lambda x:x.month)\n",
    "df['Year']=df['Date'].apply(lambda x:x.year)\n",
    "df.drop('Date', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d5a91a",
   "metadata": {},
   "source": [
    "We have splitted the Date column into day, month and year for visualisation purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1534ba96",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ed0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.figure(figsize=(8,8))\n",
    "col1 = ['Year', 'Month', 'Day', 'station', 'DEM', 'Slope']\n",
    "for i in col1:\n",
    "    pt.figure(figsize=(8,3))\n",
    "    sb.countplot(x=df[i])\n",
    "    pt.xticks(rotation=90)\n",
    "    pt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6d2e7b",
   "metadata": {},
   "source": [
    "We motice the following from above graphs-\n",
    "1) Year shows that almost all year data points have equal coverage\n",
    "2) Month shows a very high peak in data for months July and August\n",
    "3) Day shows a very high peak in data for days 7 and 8 of a month\n",
    "4) Station(25) also shows almost equal data coverage for all it's unique values\n",
    "5) DEM(25) has almost equal data coverage for all it's unique values\n",
    "6) Slope(27) again shows almost equal data coverage for all it's unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4daaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2=['Present_Tmax', 'Present_Tmin', 'LDAPS_RHmin', 'LDAPS_RHmax', 'LDAPS_Tmax_lapse', 'LDAPS_Tmin_lapse', 'LDAPS_WS', 'LDAPS_LH', 'LDAPS_CC1', 'LDAPS_CC2', 'LDAPS_CC3', 'LDAPS_CC4', 'LDAPS_PPT1', 'LDAPS_PPT2', 'LDAPS_PPT3', 'LDAPS_PPT4', 'Solar radiation', 'Next_Tmax', 'Next_Tmin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ae89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df[cols2]:\n",
    "    pt.figure(figsize=(6,3))\n",
    "    print(f\"Scatter plot for {i} column with respect to the rows covered ->\")\n",
    "    pt.scatter(df.index, df[i],color='y')\n",
    "    pt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e46058",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['station', 'Present_Tmax', 'Present_Tmin', 'LDAPS_RHmin', 'LDAPS_RHmax', 'LDAPS_Tmax_lapse', \n",
    "                   'LDAPS_Tmin_lapse', 'LDAPS_WS', 'LDAPS_LH', 'LDAPS_CC1', 'LDAPS_CC2', 'LDAPS_CC3', 'LDAPS_CC4', \n",
    "                   'LDAPS_PPT1', 'LDAPS_PPT2', 'LDAPS_PPT3', 'LDAPS_PPT4', 'DEM', 'Slope', 'Solar radiation', 'Day', \n",
    "                   'Month', 'Year']\n",
    "label_cols = ['Next_Tmax', 'Next_Tmin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775fd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in df[feature_columns]:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.lineplot(x=df[z], y=label_columns[0], data=df)\n",
    "    sns.lineplot(x=df[z], y=label_columns[1], data=df)\n",
    "    plt.ylabel(\"Labels\")\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(['Next_Tmax', 'Next_Tmin'], fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2326db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df[feature_cols]:\n",
    "    pt.figure(figsize=(6,3))\n",
    "    sb.lineplot(x=df[i], y=label_cols[0], data=df)\n",
    "    sb.lineplot(x=df[i], y=label_cols[1], data=df)\n",
    "    pt.ylabel(\"Labels\")\n",
    "    pt.xticks(fontsize=12)\n",
    "    pt.yticks(fontsize=12)\n",
    "    pt.legend(['Next_Tmax', 'Next_Tmin'], fontsize=12)\n",
    "    pt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c51c9e9",
   "metadata": {},
   "source": [
    "## Checking and removing outliers using Zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2cc916",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.figure(figsize=(20,20),facecolor='lightpink')\n",
    "p=1\n",
    "for c in df:\n",
    "    if p<=28:\n",
    "        ax=pt.subplot(6,5,p)\n",
    "        sb.boxplot(df[c],color='purple')\n",
    "        pt.xlabel(c,fontsize=15)\n",
    "    p+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baeb1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "z = np.abs(zscore(df))\n",
    "threshold = 3\n",
    "df1 = df[(z<3).all(axis = 1)]\n",
    "\n",
    "print (\"Shape of the dataframe before removing outliers: \", df.shape)\n",
    "print (\"Shape of the dataframe after removing outliers: \", df1.shape)\n",
    "print (\"Percentage of data loss post outlier removal: \", (df.shape[0]-df1.shape[0])/df.shape[0]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e887708",
   "metadata": {},
   "source": [
    "If we take threshold value f 3 the data loss is more than the acceptance range so we will increase the threshold to 3.5 to lower the dataloss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04154f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.abs(zscore(df))\n",
    "threshold = 3.5\n",
    "df1 = df[(z<3.5).all(axis = 1)]\n",
    "\n",
    "print (\"Shape of the dataframe before removing outliers: \", df.shape)\n",
    "print (\"Shape of the dataframe after removing outliers: \", df1.shape)\n",
    "print (\"Percentage of data loss post outlier removal: \", (df.shape[0]-df1.shape[0])/df.shape[0]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac67fe6",
   "metadata": {},
   "source": [
    "## Checking and removing Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eefb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.figure(figsize=(24,25),facecolor='lightblue')\n",
    "ptno=1\n",
    "\n",
    "for c in df1:\n",
    "    if ptno<=28:\n",
    "        ax=pt.subplot(6,5,ptno)\n",
    "        sb.distplot(df1[c],color='magenta',kde_kws={\"shade\": True})\n",
    "        pt.xlabel(c,fontsize=15)\n",
    "        \n",
    "    ptno+=1\n",
    "df1.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ca7e7",
   "metadata": {},
   "source": [
    "We can see that there is some skewness present in the columns , so we will remove skewness from feature columns and try to bring the skewness within the acceptance range which is (-0.5-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4357ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "yj = PowerTransformer(method = 'yeo-johnson')\n",
    "s=['LDAPS_PPT1','LDAPS_PPT2','LDAPS_PPT3','LDAPS_PPT4','DEM','Slope']\n",
    "x=df1.drop(columns=['Next_Tmin'])\n",
    "y=df1['Next_Tmin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cfdcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[s] = yj.fit_transform(df1[s].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a59b2",
   "metadata": {},
   "source": [
    "We can see that the data is less skewed now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03c5bf8",
   "metadata": {},
   "source": [
    "## Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1403c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "scaled_x = sc.fit_transform(x)\n",
    "x1=pd.DataFrame(scaled_x,columns=x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349904cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif=pd.DataFrame()\n",
    "vif['Vif values']=[variance_inflation_factor(x1.values,i)\n",
    "                    for i in range(len(x1.columns))]\n",
    "vif[\"features\"]=x1.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7007447e",
   "metadata": {},
   "source": [
    "As all the VIF values are less than 10 we can proceed with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7708abe7",
   "metadata": {},
   "source": [
    "## Selecting the best random state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05340111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "from sklearn.model_selection import train_test_split,cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b7a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_reg=[LinearRegression(),Ridge(),Lasso(),SVR(),XGBRegressor(),GradientBoostingRegressor(),RandomForestRegressor(),KNeighborsRegressor(),DecisionTreeRegressor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5ea083",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAcc = 0\n",
    "maxRS = 0\n",
    "\n",
    "for i in range(1,50):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x1,y,test_size = .25, random_state=i)\n",
    "    for m in models_reg:\n",
    "        m.fit(x_train,y_train)\n",
    "        pred = m.predict(x_test)\n",
    "        acc = r2_score(y_test,pred)\n",
    "        if acc>maxAcc:\n",
    "            maxAcc = acc\n",
    "            maxRs=i\n",
    "print(\"Best Accuracy is:\", maxAcc, \"on Random State:\", maxRs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39800e",
   "metadata": {},
   "source": [
    "So the best random state is 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3a5a7f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    " x_train,x_test,y_train,y_test = train_test_split(x1,y,test_size = .25, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f126ca",
   "metadata": {},
   "source": [
    "## Every regression model with their metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9f306606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m For LinearRegression() \u001b[0m\n",
      "R2 score : 0.84779015585185\n",
      "Mean absolute error:  0.7751272067508373\n",
      "Mean squared error:  0.9529607877749832\n",
      "Cross Validation Score= 0.8439342284353103 \n",
      "\n",
      "\u001b[1m For Ridge() \u001b[0m\n",
      "R2 score : 0.8477834574601438\n",
      "Mean absolute error:  0.7751314168084046\n",
      "Mean squared error:  0.9530027253032218\n",
      "Cross Validation Score= 0.8439349709943483 \n",
      "\n",
      "\u001b[1m For Lasso() \u001b[0m\n",
      "R2 score : 0.6074649952970268\n",
      "Mean absolute error:  1.2435302359499099\n",
      "Mean squared error:  2.457597072019265\n",
      "Cross Validation Score= 0.6059791366562861 \n",
      "\n",
      "\u001b[1m For SVR() \u001b[0m\n",
      "R2 score : 0.928950895270215\n",
      "Mean absolute error:  0.5122793502246432\n",
      "Mean squared error:  0.4448267534398246\n",
      "Cross Validation Score= 0.9164238681541909 \n",
      "\n",
      "\u001b[1m For XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...) \u001b[0m\n",
      "R2 score : 0.9446872082447164\n",
      "Mean absolute error:  0.45289541621604423\n",
      "Mean squared error:  0.3463042873484823\n",
      "Cross Validation Score= 0.9304897168037357 \n",
      "\n",
      "\u001b[1m For GradientBoostingRegressor() \u001b[0m\n",
      "R2 score : 0.904013303276049\n",
      "Mean absolute error:  0.603068406743445\n",
      "Mean squared error:  0.6009569133842813\n",
      "Cross Validation Score= 0.8920971860272096 \n",
      "\n",
      "\u001b[1m For RandomForestRegressor() \u001b[0m\n",
      "R2 score : 0.9160188255371972\n",
      "Mean absolute error:  0.5521512893982807\n",
      "Mean squared error:  0.5257923140401143\n",
      "Cross Validation Score= 0.8998509549570219 \n",
      "\n",
      "\u001b[1m For KNeighborsRegressor() \u001b[0m\n",
      "R2 score : 0.8789507650743085\n",
      "Mean absolute error:  0.6662808022922636\n",
      "Mean squared error:  0.7578693409742121\n",
      "Cross Validation Score= 0.8700713383380281 \n",
      "\n",
      "\u001b[1m For DecisionTreeRegressor() \u001b[0m\n",
      "R2 score : 0.8147862002055322\n",
      "Mean absolute error:  0.7984527220630373\n",
      "Mean squared error:  1.1595931232091692\n",
      "Cross Validation Score= 0.7704231154706855 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in models_reg:\n",
    "    m.fit(x_train,y_train)\n",
    "    mpred=m.predict(x_test)\n",
    "    print('\\033[1m','For',m,'\\033[0m')\n",
    "    print(\"R2 score :\",r2_score(y_test, mpred))\n",
    "    print(\"Mean absolute error: \", mean_absolute_error(y_test,mpred))\n",
    "    print(\"Mean squared error: \", mean_squared_error(y_test,mpred))\n",
    "    cvs=cross_val_score(m,x_train,y_train)\n",
    "    print('Cross Validation Score=',cvs.mean(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b4f43",
   "metadata": {},
   "source": [
    "## HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd7f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param = {'n_estimators':[50,60,70],'max_depth': [10,15,20],\n",
    "            'criterion':['friedman_mse', 'absolute_error', 'poisson', 'squared_error'],\n",
    "             'min_samples_split':[5,10,15,20],\n",
    "             'max_features':[\"sqrt\",\"log2\"]}\n",
    "gcs=GridSearchCV(RandomForestRegressor(),param,cv=5)\n",
    "gcs.fit(x_train,y_train)\n",
    "gcs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed5310",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel=RandomForestRegressor(max_features='sqrt',criterion='mse',min_samples_split=5,max_depth=20,n_estimators=70)\n",
    "fmodel.fit(x_train,y_train)\n",
    "pred=fmodel.predict(x_test)\n",
    "acc=r2_score(y_test,pred)\n",
    "print(acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367ed16b",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28051e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(fmodel,'Temp_Pred_Next_Tmin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a90020",
   "metadata": {},
   "source": [
    "## Now we will build the model by taking column 'Next_Tmax' as our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4579dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df1.drop(columns=['Next_Tmax'])\n",
    "y=df1['Next_Tmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f19304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "scaledx = sc.fit_transform(x)\n",
    "x2=pd.DataFrame(scaled_x,columns=x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdbb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAcc = 0\n",
    "maxRS = 0\n",
    "\n",
    "for i in range(1,50):\n",
    "    xtrain,xtest,ytrain,ytest = train_test_split(x2,y,test_size = .25, random_state=i)\n",
    "    lg = LinearRegression()\n",
    "    lg.fit(xtrain,ytrain)\n",
    "    pred = lg.predict(xtest)\n",
    "    acc = r2_score(ytest,pred)\n",
    "    if acc>maxAcc:\n",
    "        maxAcc = acc\n",
    "        maxRs=i\n",
    "print(\"Best Accuracy is:\", maxAcc, \"on Random State:\", maxRs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e0b2e",
   "metadata": {},
   "source": [
    "So the best random state is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest=train_test_split(x2,y,test_size=0.25,random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e7528",
   "metadata": {},
   "source": [
    "## Every regression model with their metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7d21ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in models_reg:\n",
    "    m.fit(x_train,y_train)\n",
    "    mpred=m.predict(x_test)\n",
    "    print('\\033[1m','For',m,'\\033[0m')\n",
    "    print(\"R2 score :\",r2_score(y_test, mpred))\n",
    "    print(\"Mean absolute error: \", mean_absolute_error(y_test,mpred))\n",
    "    print(\"Mean squared error: \", mean_squared_error(y_test,mpred))\n",
    "    cvs=cross_val_score(m,x_train,y_train)\n",
    "    print('Cross Validation Score=',cvs.mean(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04054dcb",
   "metadata": {},
   "source": [
    "## HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ed26ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param = {'n_estimators':[50,60,70],'max_depth': [10,15,20],\n",
    "            'criterion':['mse','mae'],\n",
    "             'min_samples_split':[5,10,15,20],\n",
    "             'max_features':[\"auto\",\"sqrt\",\"log2\"]}\n",
    "gcs=GridSearchCV(r,param,cv=5)\n",
    "gcs.fit(x_train,y_train)\n",
    "gcs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e1e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel=RandomForestRegressor(max_features='sqrt',criterion='mse',min_samples_split=5,max_depth=20,n_estimators=70)\n",
    "fmodel.fit(x_train,y_train)\n",
    "pred=fmodel.predict(x_test)\n",
    "acc=r2_score(y_test,pred)\n",
    "print(acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877e9f25",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b871f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(fmodel,'Temp_Pred_Next_Tmin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d995d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436b9c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa55c80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
